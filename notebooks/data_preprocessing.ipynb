{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9446ac7",
   "metadata": {},
   "source": [
    "\n",
    "# Bakery Sales Data Preprocessing\n",
    "This notebook performs data preprocessing for the Bakery Sales dataset, including:\n",
    "- Loading and inspecting the datasets (`sales` and `prices`)\n",
    "- Cleaning data by handling missing values and duplicates\n",
    "- Feature engineering for analysis and chatbot development\n",
    "- Saving cleaned data for subsequent use.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2659e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e887273",
   "metadata": {},
   "source": [
    "## Step 1: Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8250126",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define file paths (adjust paths based on project folder structure)\n",
    "sales_file = './data/raw/bakery_sales.csv'\n",
    "prices_file = './data/raw/bakery_prices.csv'\n",
    "\n",
    "# Load the datasets\n",
    "sales_df = pd.read_csv(sales_file)\n",
    "prices_df = pd.read_csv(prices_file)\n",
    "\n",
    "# Display basic info and first rows of both datasets\n",
    "print(\"Sales Data:\")\n",
    "print(sales_df.info(), \"\\n\")\n",
    "print(sales_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Prices Data:\")\n",
    "print(prices_df.info(), \"\\n\")\n",
    "print(prices_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dbace4",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12fd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handle missing values\n",
    "sales_df.dropna(inplace=True)\n",
    "prices_df.dropna(inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "sales_df.drop_duplicates(inplace=True)\n",
    "prices_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Convert datetime column to pandas datetime type\n",
    "sales_df['datetime'] = pd.to_datetime(sales_df['datetime'])\n",
    "\n",
    "# Standardize column names (if necessary)\n",
    "sales_df.columns = sales_df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "prices_df.columns = prices_df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Display cleaned data summary\n",
    "print(\"Cleaned Sales Data:\")\n",
    "print(sales_df.info(), \"\\n\")\n",
    "print(\"Cleaned Prices Data:\")\n",
    "print(prices_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb46156",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge datasets based on common identifiers (if applicable)\n",
    "merged_df = pd.merge(sales_df, prices_df, on='item_name', how='left')\n",
    "\n",
    "# Create new features (e.g., revenue per sale, total items sold per day)\n",
    "merged_df['revenue'] = merged_df['quantity'] * merged_df['price']\n",
    "merged_df['day_of_week'] = merged_df['datetime'].dt.day_name()\n",
    "\n",
    "# Aggregate data for analysis\n",
    "daily_sales = merged_df.groupby('day_of_week')['revenue'].sum().sort_values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e3470",
   "metadata": {},
   "source": [
    "## Step 4: Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1659ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create output directories if they don't exist\n",
    "output_dir = './data/processed/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save cleaned and processed data\n",
    "merged_df.to_csv(os.path.join(output_dir, 'cleaned_bakery_data.csv'), index=False)\n",
    "\n",
    "print(\"Processed data saved to:\", output_dir)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
